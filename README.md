# Build a Large Language Model (From Scratch)

> **Maintained & Customized by:** **MD Fakhruddin Razy**
> GitHub: [https://github.com/fakruddin01](https://github.com/fakruddin01)

---

This repository is a **customized fork** of the original *Build a Large Language Model (From Scratch)* project. It contains code, notes, and experiments for **learning, pretraining, and finetuning GPT-like Large Language Models (LLMs)** from the ground up.

The project follows a **from-scratch, educational approach**, focusing on understanding how LLMs work internally by implementing every core component step by step using **Python and PyTorch**.

<br>

## üìò Original Inspiration

This project is based on the book:

**Build a Large Language Model (From Scratch)**
Author: *Sebastian Raschka*
Publisher: Manning (2024)

* Book: [https://www.manning.com/books/build-a-large-language-model-from-scratch](https://www.manning.com/books/build-a-large-language-model-from-scratch)


## üöÄ What This Repository Contains

* GPT-style Transformer implementation from scratch
* Text tokenization & dataloaders
* Self-attention & multi-head attention
* GPT pretraining loop
* Finetuning for:

  * Text classification
  * Instruction following
* Experiments with:

  * LoRA
  * KV Cache
  * Mixture of Experts (MoE)
  * LLaMA-style architectures

All code is written with **clarity and learning in mind**, making it beginner-friendly.

---

## üß† Learning Roadmap

| Chapter  | Topic                       |
| -------- | --------------------------- |
| Ch 1     | Understanding LLMs          |
| Ch 2     | Working with Text Data      |
| Ch 3     | Attention Mechanisms        |
| Ch 4     | GPT from Scratch            |
| Ch 5     | Pretraining                 |
| Ch 6     | Finetuning (Classification) |
| Ch 7     | Instruction Finetuning      |
| Appendix | PyTorch, LoRA, Optimization |

---

## üõ† Tech Stack

* **Language:** Python
* **Framework:** PyTorch
* **Concepts:** Transformers, Attention, Tokenization, Training Loops
* **Hardware:** CPU / GPU (auto-detected)

---

## üß© Prerequisites

* Basic Python programming
* Interest in Machine Learning & AI
* Curiosity about how ChatGPT-like models work internally

> PyTorch experience is helpful but **not required**.

---

## üíª Hardware Requirements

This project is designed to run on:

* Regular laptops
* Google Colab
* Local GPU machines (if available)

No expensive hardware is required.

---

## üéØ Goal of This Fork

* Learn LLMs **by building them**
* Create a personal reference for future ML projects
* Experiment with modern LLM techniques
* Share clean, understandable implementations

---

## ü§ù Contributions & Feedback

This is a **personal learning repository**.

* Suggestions are welcome via GitHub Issues
* Discussions are encouraged
* Pull requests may be accepted if they improve clarity or learning

---

## üìå Author

**Fakhruddin Razy**
Software Engineering Student | ML & Web Enthusiast
GitHub: [https://github.com/fakruddin01](https://github.com/fakruddin01)

> *"If you can build it from scratch, you truly understand it."*

---

## üìö Citation (Original Work)

If you use concepts or code inspired by the original book, please cite:

> Raschka, Sebastian. *Build A Large Language Model (From Scratch)*. Manning, 2024.

---

‚≠ê If you find this repository helpful, consider giving it a star!
